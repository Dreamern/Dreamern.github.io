<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Word Embedding Embedding就是把一个东西(离散随机变量)映射到一个向量。 以颜色为例。颜色可以用RGB表示法来表示，我们可以把它拆成三个特征维度，用这三个维度的组合理论上可以表示任意一种颜色。 同理，对于词，我们也可以把它拆成指定数量的特征维度，词表中的每一个词都可以用这些维度组合成的向量来表示，这个就是Word Embedding的含义。 通常通过embedding来降维。">
<meta property="og:type" content="article">
<meta property="og:title" content="Embedding">
<meta property="og:url" content="http://example.com/2023/02/09/Embedding/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Word Embedding Embedding就是把一个东西(离散随机变量)映射到一个向量。 以颜色为例。颜色可以用RGB表示法来表示，我们可以把它拆成三个特征维度，用这三个维度的组合理论上可以表示任意一种颜色。 同理，对于词，我们也可以把它拆成指定数量的特征维度，词表中的每一个词都可以用这些维度组合成的向量来表示，这个就是Word Embedding的含义。 通常通过embedding来降维。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/02/09/Embedding/image-20230209120139317.png">
<meta property="article:published_time" content="2023-02-09T03:46:55.000Z">
<meta property="article:modified_time" content="2023-02-09T07:15:55.785Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/02/09/Embedding/image-20230209120139317.png">

<link rel="canonical" href="http://example.com/2023/02/09/Embedding/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Embedding | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/09/Embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Embedding
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-02-09 11:46:55 / Modified: 15:15:55" itemprop="dateCreated datePublished" datetime="2023-02-09T11:46:55+08:00">2023-02-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="word-embedding">Word Embedding</h1>
<p>Embedding就是把一个东西(离散随机变量)映射到一个向量<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.419ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 627.3 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mpadded"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g><g data-mml-node="mspace"></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(55.3,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></g></svg></mjx-container></span>。</p>
<p>以颜色为例。颜色可以用RGB表示法来表示，我们可以把它拆成三个特征维度，用这三个维度的组合理论上可以表示任意一种颜色。</p>
<p>同理，对于词，我们也可以把它拆成指定数量的特征维度，词表中的每一个词都可以用这些维度组合成的向量来表示，这个就是Word
Embedding的含义。</p>
<p>通常通过embedding来降维。</p>
<p>如果两个东西很像，那么得到的向量 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.407ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1063.8 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mpadded"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g><g data-mml-node="mspace"></g><g data-mml-node="msub" transform="translate(55.3,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container></span> 和 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.407ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1063.8 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mpadded"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g><g data-mml-node="mspace"></g><g data-mml-node="msub" transform="translate(55.3,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></span> 的欧式距离很小。</p>
<p>Embedding
层保存单词的<strong>分布式表示</strong>，在正向传播时，提取单词 ID
对应的向量。</p>
<blockquote>
<p>单词的分布式表示将单词表示为固定长度的密集向量。
这种向量的特征在于它是用密集向量表示的。 密集向量的意思是，
向量的各个元素（大多数）是由非0实数表示的。
例如，三维分布式表示是[0.21,-0.45,0.83] 。</p>
<p>单词的分布式表示又称为词嵌入。</p>
</blockquote>
<h1 id="nn.embedding">nn.Embedding</h1>
<p>nn.Embedding()其实是NLP中常用的词嵌入层，在实现词嵌入的过程中embedding层的权重用于随机初始化词的向量，<strong>embedding层的权重参数在后续训练时会不断更新调整，并被优化</strong>。</p>
<p>nn.Embedding是一个矩阵类，该开始时里面初始化了一个随机矩阵，矩阵的长是字典的大小，宽是用来表示字典中每个元素的属性向量，向量的维度根据你想要表示的元素的复杂度而定。类实例化之后可以根据字典中元素的下标来查找元素对应的向量。</p>
<img src="/2023/02/09/Embedding/image-20230209120139317.png" class="" title="image-20230209120139317">
<p>因为输入的句子长度不一，有的长有的短。长了截断，不够长补齐(我文中用’'填充,然后在nn.embedding层将其补0，也就是用它来表示无意义的词，这样在后面的max-pooling层也就自然而然会把其过滤掉，这样就不用担心他会影响识别。)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="literal">None</span>,</span><br><span class="line"> max_norm=<span class="literal">None</span>,  norm_type=<span class="number">2.0</span>,   scale_grad_by_freq=<span class="literal">False</span>, </span><br><span class="line"> sparse=<span class="literal">False</span>,  _weight=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 随机初始化词向量，词向量值在正态分布N(0,1)中随机取值。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入</span></span><br><span class="line">torch.nn.Embedding(</span><br><span class="line">    num_embeddings, <span class="comment"># 词典的大小尺寸，比如总共出现5000个词，那就输入5000。此时index为（0-4999）</span></span><br><span class="line">    embedding_dim,	<span class="comment"># 嵌入向量的维度，即用多少维来表示一个符号。</span></span><br><span class="line">    padding_idx=<span class="literal">None</span>, <span class="comment"># 填充id，比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。（初始化为0）</span></span><br><span class="line">    max_norm=<span class="literal">None</span>, <span class="comment"># 最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。</span></span><br><span class="line">    norm_type=<span class="number">2.0</span>, <span class="comment"># 指定利用什么范数计算，并用于对比max_norm，默认为2范数。</span></span><br><span class="line">    scale_grad_by_freq=<span class="literal">False</span> <span class="comment"># 根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False.</span></span><br><span class="line">    sparse=<span class="literal">False</span>, <span class="comment"># 若为True,则与权重矩阵相关的梯度转变为稀疏张量。</span></span><br><span class="line">    _weight=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># [规整后的句子长度，句子个数（batch_size）,词向量维度]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">将原始句子规整为同一长度：</span></span><br><span class="line"><span class="string">规整包括：句子中的符号算作词，句子结尾加的EOS也算作一个词，对于长度不足的句子，填充pad</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h2 id="使用nn.embedding">使用nn.Embedding</h2>
<p>torch.nn包下的Embedding，作为训练的一层，<strong>随模型训练</strong>得到适合的词向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立词向量层</span></span><br><span class="line">embed = torch.nn.Embedding(n_vocabulary,embedding_size)</span><br></pre></td></tr></table></figure>
<p>找到对应的词向量放进网络：词向量的输入应该是什么样子</p>
<p>实际上，上面通过随机初始化建立了词向量层后，建立了一个“二维表”，存储了词典中每个词的词向量。每个mini-batch的训练，都要从词向量表找到mini-batch对应的单词的词向量作为RNN的输入放进网络。那么怎么把mini-batch中的每个句子的所有单词的词向量找出来放进网络呢，<strong>输入是什么样子，输出是什么样子？</strong></p>
<p>首先我们知道肯定先要建立一个词典，建立词典的时候都会建立一个dict：word2id：存储单词到词典序号的映射。假设一个mini-batch如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'I am a boy.'</span>,<span class="string">'How are you?'</span>,<span class="string">'I am very lucky.'</span>]</span><br></pre></td></tr></table></figure>
<p>显然，这个mini-batch有3个句子，即batch_size=3</p>
<p>第一步首先要做的是：将句子标准化，所谓标准化，指的是：<strong>大写转小写，标点分离</strong>，这部分很简单就略过。经处理后，mini-batch变为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">'i'</span>,<span class="string">'am'</span>,<span class="string">'a'</span>,<span class="string">'boy'</span>,<span class="string">'.'</span>],</span><br><span class="line"> [<span class="string">'how'</span>,<span class="string">'are'</span>,<span class="string">'you'</span>,<span class="string">'?'</span>],</span><br><span class="line"> [<span class="string">'i'</span>,<span class="string">'am'</span>,<span class="string">'very'</span>,<span class="string">'lucky'</span>,<span class="string">'.'</span>]]</span><br></pre></td></tr></table></figure>
<p>可见，这个list的元素成了一个个list。还要做一步：将上面的三个list按单词数从多到少排列。标点也算单词。至于为什么，后面会说到。</p>
<p>那就变成了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch = [[<span class="string">'i'</span>,<span class="string">'am'</span>,<span class="string">'a'</span>,<span class="string">'boy'</span>,<span class="string">'.'</span>],</span><br><span class="line">         [<span class="string">'i'</span>,<span class="string">'am'</span>,<span class="string">'very'</span>,<span class="string">'lucky'</span>,<span class="string">'.'</span>]，</span><br><span class="line">         [<span class="string">'how'</span>,<span class="string">'are'</span>,<span class="string">'you'</span>,<span class="string">'?'</span>]]</span><br></pre></td></tr></table></figure>
<p>可见，每个句子的长度，即每个内层list的元素数为：5,5,4。这个长度也要记录。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lens = [<span class="number">5</span>,<span class="number">5</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<p>之后，为了能够处理，将batch的单词表示转为在词典中的index序号，这就是word2id的作用。转换过程很简单，假设转换之后的结果如下所示，当然这些序号是我编的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch = [[<span class="number">3</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>],</span><br><span class="line">         [<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">5</span>]，</span><br><span class="line">         [<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">7</span>]]</span><br></pre></td></tr></table></figure>
<p>同时，每个句子结尾要加EOS，假设EOS在词典中的index是1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch = [[<span class="number">3</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>],</span><br><span class="line">         [<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">5</span>,<span class="number">1</span>],</span><br><span class="line">         [<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p>那么长度要更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lens = [<span class="number">6</span>,<span class="number">6</span>,<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<p>很显然，这个mini-batch中的句子长度<strong>不一致！</strong>所以为了规整的处理，对长度不足的句子，进行填充。填充PAD假设序号是2，填充之后为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch = [[<span class="number">3</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>],</span><br><span class="line">         [<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">5</span>,<span class="number">1</span>],</span><br><span class="line">         [<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>]]</span><br></pre></td></tr></table></figure>
<p>这样就可以直接取词向量训练了吗?</p>
<p>不能！上面batch有3个样例，RNN的每一步要输入每个样例的一个单词，一次输入batch_size个样例，所以batch要按list外层是时间步数(即序列长度)，list内层是batch_size排列。即batch的维度应该是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[seq_len,batch_size]</span><br><span class="line">[seq_len,batch_size]</span><br><span class="line">[seq_len,batch_size]</span><br></pre></td></tr></table></figure>
<p>重要的问题说3遍!（维度顺序应该根据使用的模型判断，对于Transformer模型来说维度应该是[batch_size,
seq_len]，因此不需要做下一步转换）</p>
<p>怎么变换呢？变换方法可以是：使用itertools模块的zip_longest函数。而且，使用这个函数，连填充这一步都可以省略，因为这个函数可以实现填充！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch = <span class="built_in">list</span>(itertools.zip_longest(batch,fillvalue=PAD))</span><br><span class="line"><span class="comment"># fillvalue就是要填充的值，强制转成list</span></span><br></pre></td></tr></table></figure>
<p>经变换，结果应该是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch = [[<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>],</span><br><span class="line">         [<span class="number">6</span>,<span class="number">4</span>,<span class="number">5</span>],</span><br><span class="line">         [<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">         [<span class="number">6</span>,<span class="number">9</span>,<span class="number">7</span>],</span><br><span class="line">         [<span class="number">7</span>,<span class="number">5</span>,<span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>]]</span><br></pre></td></tr></table></figure>
<p>记得我们还记录了一个lens：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lens = [<span class="number">6</span>,<span class="number">6</span>,<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<p>batch还要转成LongTensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch=torch.LongTensor(batch)</span><br></pre></td></tr></table></figure>
<p>这里的batch就是词向量层的输入。</p>
<p>词向量层的输出是什么样的？</p>
<p>好了，现在使用建立了的embedding直接通过batch取词向量了，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embed_batch = embed (batch)</span><br></pre></td></tr></table></figure>
<p>假设词向量维度是6，结果是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-<span class="number">0.2699</span>,  <span class="number">0.7401</span>, -<span class="number">0.8000</span>,  <span class="number">0.0472</span>,  <span class="number">0.9032</span>, -<span class="number">0.0902</span>],</span><br><span class="line">         [-<span class="number">0.2675</span>,  <span class="number">1.8021</span>,  <span class="number">1.4966</span>,  <span class="number">0.6988</span>,  <span class="number">1.4770</span>,  <span class="number">1.1235</span>],</span><br><span class="line">         [ <span class="number">0.1146</span>, -<span class="number">0.8077</span>, -<span class="number">1.4957</span>, -<span class="number">1.5407</span>,  <span class="number">0.3755</span>, -<span class="number">0.6805</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.2675</span>,  <span class="number">1.8021</span>,  <span class="number">1.4966</span>,  <span class="number">0.6988</span>,  <span class="number">1.4770</span>,  <span class="number">1.1235</span>],</span><br><span class="line">         [ <span class="number">0.1146</span>, -<span class="number">0.8077</span>, -<span class="number">1.4957</span>, -<span class="number">1.5407</span>,  <span class="number">0.3755</span>, -<span class="number">0.6805</span>],</span><br><span class="line">         [-<span class="number">0.0387</span>,  <span class="number">0.8401</span>,  <span class="number">1.6871</span>,  <span class="number">0.3057</span>, -<span class="number">0.8248</span>, -<span class="number">0.1326</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.0387</span>,  <span class="number">0.8401</span>,  <span class="number">1.6871</span>,  <span class="number">0.3057</span>, -<span class="number">0.8248</span>, -<span class="number">0.1326</span>],</span><br><span class="line">         [-<span class="number">0.3745</span>, -<span class="number">1.9178</span>, -<span class="number">0.2928</span>,  <span class="number">0.6510</span>,  <span class="number">0.9621</span>, -<span class="number">1.3871</span>],</span><br><span class="line">         [-<span class="number">0.6739</span>,  <span class="number">0.3931</span>,  <span class="number">0.1464</span>,  <span class="number">1.4965</span>, -<span class="number">0.9210</span>, -<span class="number">0.0995</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.2675</span>,  <span class="number">1.8021</span>,  <span class="number">1.4966</span>,  <span class="number">0.6988</span>,  <span class="number">1.4770</span>,  <span class="number">1.1235</span>],</span><br><span class="line">         [-<span class="number">0.7411</span>,  <span class="number">0.7948</span>, -<span class="number">1.5864</span>,  <span class="number">0.1176</span>,  <span class="number">0.0789</span>, -<span class="number">0.3376</span>],</span><br><span class="line">         [-<span class="number">0.3745</span>, -<span class="number">1.9178</span>, -<span class="number">0.2928</span>,  <span class="number">0.6510</span>,  <span class="number">0.9621</span>, -<span class="number">1.3871</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.3745</span>, -<span class="number">1.9178</span>, -<span class="number">0.2928</span>,  <span class="number">0.6510</span>,  <span class="number">0.9621</span>, -<span class="number">1.3871</span>],</span><br><span class="line">         [-<span class="number">0.0387</span>,  <span class="number">0.8401</span>,  <span class="number">1.6871</span>,  <span class="number">0.3057</span>, -<span class="number">0.8248</span>, -<span class="number">0.1326</span>],</span><br><span class="line">         [ <span class="number">0.2837</span>,  <span class="number">0.5629</span>,  <span class="number">1.0398</span>,  <span class="number">2.0679</span>, -<span class="number">1.0122</span>, -<span class="number">0.2714</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.2837</span>,  <span class="number">0.5629</span>,  <span class="number">1.0398</span>,  <span class="number">2.0679</span>, -<span class="number">1.0122</span>, -<span class="number">0.2714</span>],</span><br><span class="line">         [ <span class="number">0.2837</span>,  <span class="number">0.5629</span>,  <span class="number">1.0398</span>,  <span class="number">2.0679</span>, -<span class="number">1.0122</span>, -<span class="number">0.2714</span>],</span><br><span class="line">         [ <span class="number">0.2242</span>, -<span class="number">1.2474</span>,  <span class="number">0.3882</span>,  <span class="number">0.2814</span>, -<span class="number">0.4796</span>,  <span class="number">0.3732</span>]]],</span><br><span class="line">       grad_fn=&lt;EmbeddingBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>维度的前两维和前面讲的是一致的。可见多了一个第三维，这就是词向量维度。所以，Embedding层的输出是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[seq_len,batch_size,embedding_size]</span><br></pre></td></tr></table></figure>
<h2 id="一些注意的点">一些注意的点</h2>
<ul>
<li>nn.Embedding的输入只能是编号，不能是隐藏变量，比如one-hot，或者其它，这种情况，可以自己建一个自定义维度的线性网络层，参数训练可以单独训练或者跟随整个网络一起训练（看实验需要）</li>
<li>如果你指定了padding_idx，注意这个padding_idx也是在num_embeddings尺寸内的，比如符号总共有500个，指定了padding_idx，那么num_embeddings应该为501</li>
<li>embedding_dim的选择要注意，根据自己的符号数量，举个例子，如果你的词典尺寸是1024，那么极限压缩（用二进制表示）也需要10维，再考虑词性之间的相关性，怎么也要在15-20维左右，虽然embedding是用来降维的，但是&gt;-
也要注意这种极限维度，结合实际情况，合理定义</li>
</ul>
<h1 id="参考来源">参考来源</h1>
<p>https://blog.csdn.net/weixin_43646592/article/details/119180298</p>
<p>https://blog.csdn.net/leitouguan8655/article/details/120204103</p>
<p>https://www.jianshu.com/p/63e7acc5e890</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/02/09/Attention/" rel="prev" title="Attention">
      <i class="fa fa-chevron-left"></i> Attention
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/02/09/torchtext/" rel="next" title="torchtext">
      torchtext <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#word-embedding"><span class="nav-number">1.</span> <span class="nav-text">Word Embedding</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#nn.embedding"><span class="nav-number">2.</span> <span class="nav-text">nn.Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8nn.embedding"><span class="nav-number">2.1.</span> <span class="nav-text">使用nn.Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E6%B3%A8%E6%84%8F%E7%9A%84%E7%82%B9"><span class="nav-number">2.2.</span> <span class="nav-text">一些注意的点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%9D%A5%E6%BA%90"><span class="nav-number">3.</span> <span class="nav-text">参考来源</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
