<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Masked Autoencoders Are Scalable Vision Learners Kaiming He∗,† Xinlei Chen∗ Saining Xie Yanghao Li Piotr Doll´ar Ross Girshick Facebook AI Research (FAIR) 摘要 本文证明了掩蔽自编码器（MAE）是一种可扩展的计算机视觉自监督学习器。我们的MAE方">
<meta property="og:type" content="article">
<meta property="og:title" content="MAE">
<meta property="og:url" content="http://example.com/2023/02/20/MAE/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Masked Autoencoders Are Scalable Vision Learners Kaiming He∗,† Xinlei Chen∗ Saining Xie Yanghao Li Piotr Doll´ar Ross Girshick Facebook AI Research (FAIR) 摘要 本文证明了掩蔽自编码器（MAE）是一种可扩展的计算机视觉自监督学习器。我们的MAE方">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/02/20/MAE/image-20230313101100245.png">
<meta property="og:image" content="http://example.com/2023/02/20/MAE/image-20230314100833023.png">
<meta property="article:published_time" content="2023-02-20T08:03:26.000Z">
<meta property="article:modified_time" content="2023-04-23T09:09:59.025Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/02/20/MAE/image-20230313101100245.png">

<link rel="canonical" href="http://example.com/2023/02/20/MAE/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>MAE | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/20/MAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MAE
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-02-20 16:03:26" itemprop="dateCreated datePublished" datetime="2023-02-20T16:03:26+08:00">2023-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-04-23 17:09:59" itemprop="dateModified" datetime="2023-04-23T17:09:59+08:00">2023-04-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="masked-autoencoders-are-scalable-vision-learners">Masked
Autoencoders Are Scalable Vision Learners</h1>
<p>Kaiming He∗,† Xinlei Chen∗ Saining Xie Yanghao Li Piotr Doll´ar Ross
Girshick</p>
<p>Facebook AI Research (FAIR)</p>
<h1 id="摘要">摘要</h1>
<p>本文证明了掩蔽自编码器（MAE）是一种可扩展的计算机视觉自监督学习器。我们的MAE方法很简单：我们掩码输入图像的随机patches并重建丢失的像素。它基于两个核心设计：</p>
<ul>
<li>首先，我们开发了一个非对称的编码器-解码器架构，其中编码器只对可见的patches子集进行操作（即没有掩码标记的patches），同时还有一个轻量级的解码器，它从隐表达和掩码tokens中重建原始图像。</li>
<li>第二，我们发现对输入图像做高比例的掩码，如75%，产生了一个有意义的自监督任务。</li>
</ul>
<p>将这两种设计结合起来，我们可以高效地训练大型模型：我们加快了训练（3倍或更多）并提高了准确性。我们的可扩展方法允许学习泛化良好的高容量模型：例如，在仅使用ImageNet-1K数据的方法中，普通ViT-Huge模型实现了最佳准确度（87.8%）。下游任务的迁移性能优于有监督的预训练，并显示出良好的扩展行为。</p>
<p>image -&gt; patch embedding -&gt; + mask -&gt; encoder -&gt; +
padding -&gt; decoder -&gt; loss</p>
<img src="/2023/02/20/MAE/image-20230313101100245.png" class="" title="image-20230313101100245">
<h1 id="introduction">1. Introduction</h1>
<p>深度学习的能力和容量在不断增强，很容易在millions图像上过拟合，需要hundreds
of millions的labeled 图像来训练。</p>
<p>这种需求被NLP中的自监督预训练解决了。解决方案基于GPT的自回归和BERT的masked
autoencoding：移除一部分数据然后预测被移除的内容。</p>
<p>These methods now enable training of generalizable NLP models。</p>
<p>这种masked
autoencoders的想法天然适用于CV。BERT中做了一些预研，但CV中的进展落后于NLP。提问：是什么造成了masked
autoencoding 在CV和NLP中的表现差异？我们尝试从以下几个角度回答：</p>
<p>（1）架构不同，CV用CNN，这个问题被ViT解决了。</p>
<p>（2）信息密度不同。语言有很高的语义和信息密度，而图像有很高的空间冗余度，可利用的信息很多。因此实验中用了很高的mask比例</p>
<p>（3）decoder在CV和NLP中的角色不同，一个重构文本（包括丰富的语义信息），一个重构像素（更低级的语义水平）。在NLP中可以trivial（如MLP）。对于images，decoder的设计很关键。</p>
<p>基于以上分析，我们提出了MAE。随机mask patches，重构missing
pixel。encoder作用于visible patches without mask
tokens，轻量级decoder作用于latent representation along with mask
tokens。将mask
tokens迁移到decoder大幅降低了计算量。这种设计达到了双赢：75%的mask比例在提高准确率的同时降低3倍的预训练时间和显存消耗。</p>
<p>MAE泛化很好。在数据集上表现优异。分类、目标检测、实体分割、语义分割。</p>
<h1 id="related-work">2. Related Work</h1>
<p>Masked language modeling: BERT and GPT in NLP</p>
<p>Autoencoding:</p>
<p>Masked image encoding: ViT, BEiT</p>
<p>Self-supervised learning:</p>
<h1 id="approach">3. Approach</h1>
<p>介绍整体结构</p>
<p>Masking: mask任务</p>
<p>MAE encoder: ViT</p>
<p>MAE decoder: encode visible patches + mask tokens. The decoder has
another series of Transformer blocks. decoder
仅在预训练中用来重构图像。</p>
<p>Reconstruction target: 最后一层是linear
projection，来预测像素。使用MSE
loss。仅计算mask位置的loss。我们还尝试了利用归一化像素值来作为重构目标，这提升了表达质量。</p>
<p>Simple implementation: 数据流动过程</p>
<h1 id="imagenet-experiments">4. ImageNet Experiments</h1>
<p>Baseline:</p>
<h2 id="main-properties">4.1 Main Properties</h2>
<p>ablate</p>
<p>Masking ratio: 最优Masking
ratio非常高，约为75%，与BERT不同，约为15%</p>
<p>Decoder design: block深度和特征宽度。Note that a single Transformer
block is the minimal requirement to propagate information from visible
tokens to mask tokens.Such a small decoder can further speed up
training.</p>
<p>Mask token: An important design of our MAE is to skip the mask token
[M] in the encoder and apply it later in the lightweight decoder. this
encoder has a large portion of mask tokens in its input in pretraining,
which does not exist in uncorrupted images. This gap may degrade
accuracy in deployment. Moreover, by skipping the mask token in the
encoder, we greatly reduce training computation.</p>
<p>Reconstruction target:
对比不同的重构目标。PCA、预测tokens(BEiT)等</p>
<p>Data augmentation: 剪切、旋转等</p>
<p>Mask sampling strategy: block-wise(移除大块),
grid-wise(均匀移除4块中的1块), random表现最好</p>
<p>Training schedule</p>
<h2 id="comparisons-with-previous-results">4.2 Comparisons with Previous
Results</h2>
<p>self-supervised methods: ViT BEiT</p>
<p>supervised pre-training:</p>
<h2 id="partial-fine-tuning">4.3 Partial fine-tuning</h2>
<p>ﬁne-tune the last several layers while freezing the others.</p>
<h1 id="代码">代码</h1>
<h2 id="block">Block</h2>
<img src="/2023/02/20/MAE/image-20230314100833023.png" class="" title="image-20230314100833023">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, </span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                 drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, </span></span><br><span class="line"><span class="params">                 init_values=<span class="literal">None</span>, act_layer=nn.GELU, norm_layer=nn.LayerNorm,</span></span><br><span class="line"><span class="params">                 attn_head_dim=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(</span><br><span class="line">            dim, num_heads=num_heads, qkv_bias=qkv_bias, </span><br><span class="line">            qk_scale=qk_scale,</span><br><span class="line">            attn_drop=attn_drop, proj_drop=drop, </span><br><span class="line">            attn_head_dim=attn_head_dim</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(</span><br><span class="line">            in_features=dim,   </span><br><span class="line">            hidden_features=mlp_hidden_dim, </span><br><span class="line">            act_layer=act_layer, </span><br><span class="line">            drop=drop</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> init_values &gt; <span class="number">0</span>:	<span class="comment"># 0.0</span></span><br><span class="line">            self.gamma_1 = nn.Parameter(</span><br><span class="line">                init_values * torch.ones((dim)),requires_grad=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">            self.gamma_2 = nn.Parameter(</span><br><span class="line">                init_values * torch.ones((dim)),requires_grad=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.gamma_1, self.gamma_2 = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.gamma_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">            x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))</span><br><span class="line">            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="pretrain">Pretrain</h2>
<h3 id="encoder">Encoder</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PretrainVisionTransformerEncoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, </span></span><br><span class="line"><span class="params">        num_classes=<span class="number">0</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">        num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">        qk_scale=<span class="literal">None</span>, drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        drop_path_rate=<span class="number">0.</span>, norm_layer=nn.LayerNorm, init_values=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        use_learnable_pos_emb=<span class="literal">False</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">    	...</span><br><span class="line">        <span class="comment"># dpr = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)]</span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block(</span><br><span class="line">                dim=embed_dim, 				<span class="comment"># 768</span></span><br><span class="line">                num_heads=num_heads, 		<span class="comment"># 12</span></span><br><span class="line">                mlp_ratio=mlp_ratio, 		<span class="comment"># 4</span></span><br><span class="line">                qkv_bias=qkv_bias, 			<span class="comment"># True</span></span><br><span class="line">                qk_scale=qk_scale,			<span class="comment"># None</span></span><br><span class="line">                drop=drop_rate, 			<span class="comment"># 0.0</span></span><br><span class="line">                attn_drop=attn_drop_rate, 	<span class="comment"># 0.0</span></span><br><span class="line">                drop_path=dpr[i], 			<span class="comment"># 0.0</span></span><br><span class="line">                norm_layer=norm_layer,		<span class="comment"># nn.LayerNorm</span></span><br><span class="line">                init_values=init_values		<span class="comment"># 0.0</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)]			<span class="comment"># depth = 12</span></span><br><span class="line">        )</span><br><span class="line">    	...</span><br><span class="line">        self.head = nn.Linear(</span><br><span class="line">            embed_dim, num_classes</span><br><span class="line">        ) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        x = self.patch_embed(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># cls_tokens = self.cls_token.expand(batch_size, -1, -1) </span></span><br><span class="line">        <span class="comment"># x = torch.cat((cls_tokens, x), dim=1)</span></span><br><span class="line">        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()</span><br><span class="line"></span><br><span class="line">        B, _, C = x.shape                   <span class="comment"># x: [128, 196, 768]    mask: [128, 196]</span></span><br><span class="line">        x_vis = x[~mask].reshape(B, -<span class="number">1</span>, C) <span class="comment"># ~mask means visible</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x_vis = blk(x_vis)</span><br><span class="line"></span><br><span class="line">        x_vis = self.norm(x_vis)</span><br><span class="line">        <span class="keyword">return</span> x_vis</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        x = self.forward_features(x, mask)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="decoder">Decoder</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PretrainVisionTransformerDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">""" Vision Transformer with support for patch or hybrid CNN input stage</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, patch_size=<span class="number">16</span>, num_classes=<span class="number">768</span>, </span></span><br><span class="line"><span class="params">        embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">        num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">        qk_scale=<span class="literal">None</span>, drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        drop_path_rate=<span class="number">0.</span>, norm_layer=nn.LayerNorm, </span></span><br><span class="line"><span class="params">        init_values=<span class="literal">None</span>, num_patches=<span class="number">196</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="keyword">assert</span> num_classes == <span class="number">3</span> * patch_size ** <span class="number">2</span></span><br><span class="line">        self.num_features = self.embed_dim = embed_dim  <span class="comment"># num_features for consistency with other models</span></span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line"></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, depth)]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            Block(</span><br><span class="line">                dim=embed_dim, 				<span class="comment"># 384</span></span><br><span class="line">                num_heads=num_heads, 		<span class="comment"># 6</span></span><br><span class="line">                mlp_ratio=mlp_ratio, 		<span class="comment"># 4</span></span><br><span class="line">                qkv_bias=qkv_bias, 			<span class="comment"># True</span></span><br><span class="line">                qk_scale=qk_scale,			<span class="comment"># None</span></span><br><span class="line">                drop=drop_rate, 			<span class="comment"># 0.0</span></span><br><span class="line">                attn_drop=attn_drop_rate, 	<span class="comment"># 0.0</span></span><br><span class="line">                drop_path=dpr[i], 			<span class="comment"># 0.0</span></span><br><span class="line">                norm_layer=norm_layer,		<span class="comment"># nn.LayerNorm</span></span><br><span class="line">                init_values=init_values		<span class="comment"># 0.0</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])     <span class="comment"># depth = 4</span></span><br><span class="line">        self.norm =  norm_layer(embed_dim)</span><br><span class="line">        self.head = nn.Linear(embed_dim, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, return_token_num</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x = blk(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> return_token_num &gt; <span class="number">0</span>:</span><br><span class="line">            x = self.head(self.norm(x[:, -return_token_num:])) <span class="comment"># only return the mask tokens predict pixels</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.head(self.norm(x)) <span class="comment"># [B, N, 3*16^2]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="model">model</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br></pre></td><td class="code"><pre><span class="line">PretrainVisionTransformer(</span><br><span class="line">  (encoder): PretrainVisionTransformerEncoder(</span><br><span class="line">    (patch_embed): PatchEmbed(</span><br><span class="line">      (proj): Conv2d(<span class="number">3</span>, <span class="number">768</span>, kernel_size=(<span class="number">16</span>, <span class="number">16</span>), stride=(<span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">    )</span><br><span class="line">    (blocks): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">1</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">2</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">3</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">4</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">5</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">6</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">7</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">8</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">9</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">10</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">11</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (head): Identity()</span><br><span class="line">  )</span><br><span class="line">  (decoder): PretrainVisionTransformerDecoder(</span><br><span class="line">    (blocks): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1152</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1536</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">1536</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">1</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1152</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1536</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">1536</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">2</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1152</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1536</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">1536</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">3</span>): Block(</span><br><span class="line">        (norm1): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (attn): Attention(</span><br><span class="line">          (qkv): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1152</span>, bias=<span class="literal">False</span>)</span><br><span class="line">          (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">          (proj): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        (drop_path): Identity()</span><br><span class="line">        (norm2): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Mlp(</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">1536</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (act): GELU()</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">1536</span>, out_features=<span class="number">384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): LayerNorm((<span class="number">384</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (head): Linear(in_features=<span class="number">384</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (encoder_to_decoder): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">384</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="finetune">Finetune</h2>
<h3 id="model-1">model</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br></pre></td><td class="code"><pre><span class="line">VisionTransformer(</span><br><span class="line">  (patch_embed): PatchEmbed(</span><br><span class="line">    (proj): Conv2d(<span class="number">3</span>, <span class="number">768</span>, kernel_size=(<span class="number">16</span>, <span class="number">16</span>), stride=(<span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">  )</span><br><span class="line">  (pos_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  (blocks): ModuleList(</span><br><span class="line">    (<span class="number">0</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): Identity()</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.00909090880304575</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.0181818176060915</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">3</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.027272727340459824</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">4</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.036363635212183</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">5</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.045454543083906174</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">6</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.054545458406209946</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">7</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.06363636255264282</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">8</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.0727272778749466</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">9</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.08181818574666977</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">10</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.09090909361839294</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">11</span>): Block(</span><br><span class="line">      (norm1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (attn): Attention(</span><br><span class="line">        (qkv): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">2304</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        (attn_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">        (proj): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (proj_drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">      (drop_path): DropPath(p=<span class="number">0.10000000149011612</span>)</span><br><span class="line">      (norm2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (act): GELU()</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (drop): Dropout(p=<span class="number">0.0</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (norm): Identity()</span><br><span class="line">  (fc_norm): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-06</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">  (head): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/02/15/dataset/" rel="prev" title="dataset">
      <i class="fa fa-chevron-left"></i> dataset
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/06/24/QuantitativeTrading/" rel="next" title="QuantitativeTrading">
      QuantitativeTrading <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#masked-autoencoders-are-scalable-vision-learners"><span class="nav-number">1.</span> <span class="nav-text">Masked
Autoencoders Are Scalable Vision Learners</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">3.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#related-work"><span class="nav-number">4.</span> <span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#approach"><span class="nav-number">5.</span> <span class="nav-text">3. Approach</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#imagenet-experiments"><span class="nav-number">6.</span> <span class="nav-text">4. ImageNet Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#main-properties"><span class="nav-number">6.1.</span> <span class="nav-text">4.1 Main Properties</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#comparisons-with-previous-results"><span class="nav-number">6.2.</span> <span class="nav-text">4.2 Comparisons with Previous
Results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#partial-fine-tuning"><span class="nav-number">6.3.</span> <span class="nav-text">4.3 Partial fine-tuning</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">7.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#block"><span class="nav-number">7.1.</span> <span class="nav-text">Block</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pretrain"><span class="nav-number">7.2.</span> <span class="nav-text">Pretrain</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder"><span class="nav-number">7.2.1.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder"><span class="nav-number">7.2.2.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-number">7.2.3.</span> <span class="nav-text">model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#finetune"><span class="nav-number">7.3.</span> <span class="nav-text">Finetune</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-1"><span class="nav-number">7.3.1.</span> <span class="nav-text">model</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
